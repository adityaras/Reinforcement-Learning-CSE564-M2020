{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOiZAA027RdlsBWyrswg/JA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityaras/Aditya_2018273_RL-M2020/blob/master/HW2/RL_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KNV9MmA3JJn"
      },
      "source": [
        "# The Following Libraries have been imported\n",
        "#   1. Numpy: Used throughout the file, for random numbers, solving equations, \n",
        "#      and initializing 2-D Matrices\n",
        "#   2. Copy: To deepcopy values (In Iterative Algorithms)\n",
        "\n",
        "import numpy as np\n",
        "import copy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NndYoXTO2zpg"
      },
      "source": [
        "##Question 2: Gridworld Problem - Solving a System of Linear Equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xsXTNbDfLO1"
      },
      "source": [
        "The function **next_step** takes in two arguments\n",
        "  1.   state: The state argument is of the form (i,j) where i and j represents the row and column respectively.\n",
        "  2.   action: The action argument is also a tuple which can be one of the four allowed actions in the problem i.e. (North, South, East and West).\n",
        "\n",
        "According to the conditions i.e. taking care of out of bound conditions, and the special states given A and B, the next state S' and also the corresponding reward is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-fwezicA1Uz"
      },
      "source": [
        "def next_step(state, action):\n",
        "  if state == (0,1):\n",
        "    return (4,1), 10\n",
        "  elif state == (0,3):\n",
        "    return (2,3), 5\n",
        "  else:\n",
        "    if ((state[0]+action[0])>=0 and (state[0]+action[0])<=4 and (state[1]+action[1])>=0 and (state[1]+action[1])<=4):\n",
        "      return (state[0]+action[0],state[1]+action[1]), 0\n",
        "    else:\n",
        "      return state, -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls797DsJtTW2"
      },
      "source": [
        "The function **gridworld_prob_v_pi_s** sets up the coefficient matrix and constant matrix for the system of equations $v_{\\pi}(s)$ and $v_{\\pi}(s')$. The system of equations in represented by:\n",
        "  \n",
        "*   $v_{\\pi}(s) = \\Sigma_{a} \\pi(a|s) [\\Sigma_{s',r} p(s',r|s,a)[ r + \\gamma v_{\\pi}(s')]]$\n",
        "\n",
        "The Coefficient Matrix is initialised as (states $ \\times $ states) identity matrix. The Constant Matrix is initialised as a Zero Matrix of size (states $ \\times $ 1) .\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUCy4j2R3Enq"
      },
      "source": [
        "def gridworld_prob_v_pi_s(gamma, grid_rows, grid_cols, actions, pi):\n",
        "  states = int(grid_rows * grid_cols)\n",
        "  coeff_matrix = np.identity(states)\n",
        "  const_matrix = np.zeros((states,1))\n",
        "  curr_state = -1\n",
        "  for i in range(grid_rows):\n",
        "    for j in range(grid_cols):\n",
        "      curr_state += 1\n",
        "      for a in actions:\n",
        "        s_dash, r = next_step((i,j),a)\n",
        "        coeff_matrix[curr_state][s_dash[1]+(grid_rows*s_dash[0])] -= pi*gamma\n",
        "        const_matrix[curr_state] += pi*r\n",
        "  v_pi_s = np.linalg.solve(coeff_matrix, const_matrix)\n",
        "  v_pi_s = v_pi_s.reshape((grid_rows,grid_cols))\n",
        "\n",
        "  return v_pi_s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaW2dyvgEB_7"
      },
      "source": [
        "gamma = 0.9\n",
        "grid_rows = 5\n",
        "grid_cols = 5\n",
        "actions = [(1,0),(0,1),(-1,0),(0,-1)]\n",
        "pi = 0.25\n",
        "v_pi_s = gridworld_prob_v_pi_s(gamma, grid_rows, grid_cols, actions, pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4scxifBEmAV",
        "outputId": "80e7bb41-d793-416d-e9d6-1d352f2140fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print((v_pi_s))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3.30899634  8.78929186  4.42761918  5.32236759  1.49217876]\n",
            " [ 1.52158807  2.99231786  2.25013995  1.9075717   0.54740271]\n",
            " [ 0.05082249  0.73817059  0.67311326  0.35818621 -0.40314114]\n",
            " [-0.9735923  -0.43549543 -0.35488227 -0.58560509 -1.18307508]\n",
            " [-1.85770055 -1.34523126 -1.22926726 -1.42291815 -1.97517905]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sFPXSv-RWyl"
      },
      "source": [
        "## Question 4: Optimal State Value and Policy Functions - Solving a System of Non-Linear Equations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGQ8mWLeBrG-"
      },
      "source": [
        "The System of Non Linear Equations in the example is approximated using Value Iteration. A threshold value, Theta ( $\\theta$ ) set to 1e-4 which acts as a stopping criterion for the iteration process.\n",
        "\n",
        "The Optiml Policy Function takes into account that there might be more than one optimal action and the final optimal Policy is created with keeping this is mind. For every action the action-value function $q(s,a)$ is calculated. From these $q(s,a)$ values $a_{*}$ is calculated for each state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6MqNSzSRV0R"
      },
      "source": [
        "def grid_world_problem_optimal(gamma, grid_rows, grid_cols, actions, theta):\n",
        "  v_s_i = np.zeros((grid_cols,grid_rows))\n",
        "  v_s_i1 = np.zeros((grid_cols,grid_rows))\n",
        "  curr_state = -1\n",
        "  while True:\n",
        "    for i in range(grid_rows):\n",
        "      for j in range(grid_cols):\n",
        "        max_v_s = []\n",
        "        for a in actions:\n",
        "          s_dash, r = next_step((i,j),a)\n",
        "          v_s = r + gamma*v_s_i[s_dash[0]][s_dash[1]]\n",
        "          max_v_s.append(v_s)\n",
        "        v_s_i1[i][j] = max(max_v_s) \n",
        "    delta = abs(v_s_i1 - v_s_i).max()\n",
        "    # print(delta)\n",
        "    if delta<theta:\n",
        "      return v_s_i1\n",
        "    else:\n",
        "      v_s_i = copy.deepcopy(v_s_i1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xThOhYvAe9Qn"
      },
      "source": [
        "gamma = 0.9\n",
        "grid_rows = 5\n",
        "grid_cols = 5\n",
        "actions = [(1,0),(0,1),(-1,0),(0,-1)]\n",
        "theta = 0.00001\n",
        "v_star = grid_world_problem_optimal(gamma, grid_rows, grid_cols, actions, theta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R2B5PArfQmI",
        "outputId": "6ad70cb0-9576-49c7-e1ee-cc5f4ec8ac51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "print(v_star)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[21.97747068 24.41941186 21.97747068 19.41941186 17.47747068]\n",
            " [19.77972361 21.97747068 19.77972361 17.80174304 16.02156873]\n",
            " [17.80174304 19.77972361 17.80174304 16.02156873 14.41941186]\n",
            " [16.02156873 17.80174304 16.02156873 14.41941186 12.97747068]\n",
            " [14.41941186 16.02156873 14.41941186 12.97747068 11.67972361]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw-q-2xdvkoa"
      },
      "source": [
        "def grid_world_problem_optimal_policy(actions, grid_rows, grid_cols,v_star,dir):\n",
        "  state = -1\n",
        "  for i in range(grid_rows):\n",
        "      for j in range(grid_cols):\n",
        "        state += 1\n",
        "        q_s_a = np.zeros(len(actions))\n",
        "        k = 0\n",
        "        for a in actions:\n",
        "            s_dash, r = next_step((i,j), a)\n",
        "            q_s_a[k] = r + (gamma * v_star[s_dash[0]][s_dash[1]])\n",
        "            k += 1\n",
        "        a_star = max(q_s_a)\n",
        "        s = \"\"\n",
        "        for q in range(len(q_s_a)):\n",
        "          if a_star == q_s_a[q]:\n",
        "            s += dir[q] + \"+\"\n",
        "        s = s[:-1]\n",
        "        print(s, end=\"\")\n",
        "        if (state+1)%5 == 0:\n",
        "          print('\\n')\n",
        "        else:\n",
        "          print(\"; \", end =\"\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9BFTmrozQ_l",
        "outputId": "70fcc9f6-485e-4a24-8ffe-e2c7c18c33cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "gamma = 0.9\n",
        "grid_rows = 5\n",
        "grid_cols = 5\n",
        "actions = [(1,0),(0,1),(-1,0),(0,-1)]\n",
        "dir = {0:\"South\", 1:\"East\", 2:\"North\", 3:\"West\"}\n",
        "grid_world_problem_optimal_policy(actions, grid_rows, grid_cols,v_star,dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "East; South+East+North+West; West; South+East+North+West; West\n",
            "\n",
            "East+North; North; North+West; West; West\n",
            "\n",
            "East+North; North; North+West; North+West; North+West\n",
            "\n",
            "East+North; North; North+West; North+West; North+West\n",
            "\n",
            "East+North; North; North+West; North+West; North+West\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEDPWXRo-73a"
      },
      "source": [
        "## Question 6: Policy and Value Iteration in the Gridworld Example 4.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6OsD3wVBojJ"
      },
      "source": [
        "### Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBNr_I6cISb2"
      },
      "source": [
        "Firstly a new **next_step_ex41** is implemented according to the new Grid World Conditions, which is very similar to the one implemented above but also takes an additional argument, *grid_rows* to take care of terminal states.\n",
        "\n",
        "For Policy Iteration,\n",
        "Two functions are implemented: Followed Instructions provided in the Textbook (Sutton)\n",
        "\n",
        "\n",
        "1.   Policy Evaluation\n",
        "2.   Policy Improvement\n",
        "\n",
        "The Bug mentioned in Exercise 4.4 will not take place because we take all possible maximum values and update them simultaneously. As a result the bug that the policy may oscillate between two maximum values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDT5IcaU-6P6"
      },
      "source": [
        "def next_step_ex41(state, action, grid_rows):\n",
        "  if ((state[0]+action[0])>=0 and (state[0]+action[0])<4 and (state[1]+action[1])>=0 and (state[1]+action[1])<4):\n",
        "    if (state[0]+action[0], state[1]+action[1]) == (0,0) or (state[0]+action[0], state[1]+action[1]) == (grid_rows-1, grid_rows-1):\n",
        "      return (grid_rows-1,grid_cols-1), -1\n",
        "    else:\n",
        "      return (state[0]+action[0],state[1]+action[1]), -1\n",
        "  else:\n",
        "    return state, -1"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLNcnfzvifcl"
      },
      "source": [
        "def grid_world_4x4_policy_eval(v_pi, grid_rows, grid_cols, actions, pi_mat,term_state, theta):\n",
        "  # v_pi = np.zeros((grid_rows,grid_cols))\n",
        "  while(True):\n",
        "    delta = 0\n",
        "    for i in range(grid_rows):\n",
        "      for j in range(grid_cols):\n",
        "        if (i,j) not in term_state:\n",
        "          v = copy.deepcopy(v_pi[i][j])\n",
        "          val = 0\n",
        "          c = 0\n",
        "          for a in actions:\n",
        "            s_dash, r = next_step_ex41((i,j), a, grid_rows)\n",
        "            val += pi_mat[i][j][c] * (r + v_pi[s_dash[0]][s_dash[1]])\n",
        "            c += 1\n",
        "          v_pi[i][j] = val\n",
        "          delta = max(delta, abs(v_pi[i][j] - v))\n",
        "          # print(\"Delta \"+str(delta))\n",
        "    if delta < theta:\n",
        "      return v_pi\n",
        "    "
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izH4-NdXbPi3"
      },
      "source": [
        "grid_rows = 4\n",
        "grid_cols = 4\n",
        "v_pi = np.zeros((grid_rows,grid_cols))\n",
        "actions = [(1,0),(0,1),(-1,0),(0,-1)]\n",
        "pi = 0.25\n",
        "theta = 0.00001\n",
        "term_state = [(0,0),(grid_rows-1,grid_cols-1)]\n",
        "dir = {0:\"South\", 1:\"East\", 2:\"North\", 3:\"West\"}\n",
        "v_pi = grid_world_4x4_policy_eval(v_pi,grid_rows, grid_cols, actions, pi,term_state, theta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d8EvaVxcTPo",
        "outputId": "4930edb4-8e64-48a9-8ff0-1c70d7f02aca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(v_pi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0.         -13.99993529 -19.99990698 -21.99989761]\n",
            " [-13.99993529 -17.9999206  -19.99991379 -19.99991477]\n",
            " [-19.99990698 -19.99991379 -17.99992725 -13.99994569]\n",
            " [-21.99989761 -19.99991477 -13.99994569   0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wenO9777dQaG"
      },
      "source": [
        "def grid_world_4x4_policy_improv(pi_mat,v_pi,grid_rows, grid_cols, actions, pi,term_state, action_map):\n",
        "  no_actions = len(actions)\n",
        "  # pi_mat = np.full((grid_rows,grid_cols,no_actions),pi)\n",
        "  policy_stable = True\n",
        "  for i in range(grid_rows):\n",
        "    for j in range(grid_cols):\n",
        "      if (i,j) not in term_state:\n",
        "        old_a = copy.deepcopy(pi_mat[i][j])\n",
        "        a_ind = 0\n",
        "        returns = []\n",
        "        for a in actions:\n",
        "          s_dash, r = next_step_ex41((i,j), a, grid_rows)\n",
        "          returns.append(r + v_pi[s_dash[0]][s_dash[1]])\n",
        "        max_val = max(returns)\n",
        "        max_a = []\n",
        "        for  ret in range(len(returns)):\n",
        "          if returns[ret] == max_val:\n",
        "            max_a.append(ret)\n",
        "        pi_mat[i][j] = [0.] * no_actions\n",
        "        for a_ind in max_a:\n",
        "          pi_mat[i][j][a_ind] = (1/len(max_a))\n",
        "        for ind in range(len(old_a)):\n",
        "          if (old_a[ind] != pi_mat[i][j][ind]):\n",
        "            policy_stable = False\n",
        "            break\n",
        "  return pi_mat, policy_stable          "
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKUvuBagjsS_",
        "outputId": "e5239cdf-7461-4e5a-b57a-08e3a7d9e023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "grid_rows = 4\n",
        "grid_cols = 4\n",
        "actions = [(1,0),(0,1),(-1,0),(0,-1)]\n",
        "action_map = {0:(1,0),1:(0,1),2:(-1,0),3:(0,-1)}\n",
        "pi = 0.25\n",
        "term_state = [(0,0),(grid_rows-1,grid_cols-1)]\n",
        "dir = {0:\"D\", 1:\"R\", 2:\"U\", 3:\"L\"}\n",
        "no_actions = len(actions)\n",
        "pi_mat = np.full((grid_rows,grid_cols,no_actions),pi)\n",
        "print(grid_world_4x4_policy_improv(pi_mat,v_pi,grid_rows, grid_cols, actions, pi,term_state, action_map))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([[[0.25, 0.25, 0.25, 0.25],\n",
            "        [0.  , 0.  , 0.  , 1.  ],\n",
            "        [0.  , 0.  , 0.  , 1.  ],\n",
            "        [0.  , 0.  , 0.  , 1.  ]],\n",
            "\n",
            "       [[0.  , 0.  , 1.  , 0.  ],\n",
            "        [0.  , 0.  , 0.5 , 0.5 ],\n",
            "        [0.  , 0.  , 0.  , 1.  ],\n",
            "        [1.  , 0.  , 0.  , 0.  ]],\n",
            "\n",
            "       [[0.  , 0.  , 1.  , 0.  ],\n",
            "        [0.  , 0.  , 1.  , 0.  ],\n",
            "        [0.  , 1.  , 0.  , 0.  ],\n",
            "        [1.  , 0.  , 0.  , 0.  ]],\n",
            "\n",
            "       [[0.  , 0.  , 1.  , 0.  ],\n",
            "        [0.  , 1.  , 0.  , 0.  ],\n",
            "        [0.  , 1.  , 0.  , 0.  ],\n",
            "        [0.25, 0.25, 0.25, 0.25]]]), False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q-oTKKTB-Oz"
      },
      "source": [
        "def fin_policy(pi_mat,dir,grid_rows,grid_cols,term_state):\n",
        "  for i in range(grid_rows):\n",
        "    for j in range(grid_cols):\n",
        "      if (i,j) not in term_state:\n",
        "        val = max(pi_mat[i][j])\n",
        "        s=\"\"\n",
        "        for k in range(len(pi_mat[i][j])):\n",
        "          if val == pi_mat[i][j][k]:\n",
        "            s+=dir[k]\n",
        "        print(s,end=\"\")\n",
        "      else:\n",
        "        print(\"#\",end=\"\")\n",
        "      if (j==grid_cols-1):\n",
        "        print(\"\\n\")\n",
        "      else:\n",
        "        print(\";\",end=\" \")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ0XbH3bvAqH"
      },
      "source": [
        "def grid_4x4_PI(grid_rows, grid_cols, actions, pi,term_state, theta, action_map,dir):\n",
        "  i = 0\n",
        "  v_pi = np.zeros((grid_rows,grid_cols))\n",
        "  no_actions = len(actions)\n",
        "  pi_mat = np.full((grid_rows,grid_cols,no_actions),pi)\n",
        "  while True:\n",
        "    print(\"Iteration \"+str(i))\n",
        "    print(\"Evaluating Policy...\")\n",
        "\n",
        "    v = grid_world_4x4_policy_eval(v_pi, grid_rows, grid_cols, actions, pi_mat,term_state, theta)\n",
        "    v_pi = copy.deepcopy(v)\n",
        "    print(v_pi)\n",
        "\n",
        "    print(\"Improving Policy...\")\n",
        "\n",
        "    pi_mat_temp, stable = grid_world_4x4_policy_improv(pi_mat,v_pi,grid_rows, grid_cols, actions, pi,term_state, action_map)\n",
        "    pi_mat = copy.deepcopy(pi_mat_temp)\n",
        "    print(pi_mat)\n",
        "\n",
        "    if stable:\n",
        "      print(\"\\nPolicy Converged\\n\")\n",
        "      fin_policy(pi_mat,dir,grid_rows,grid_cols,term_state)\n",
        "      break\n",
        "    i+=1"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNpSuKcqzEUj",
        "outputId": "d477a9bd-c9d1-4950-f3aa-0a06ac8d3c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "grid_rows = 4\n",
        "grid_cols = 4\n",
        "actions = [(1,0),(0,1),(-1,0),(0,-1)]\n",
        "action_map = {0:(1,0),1:(0,1),2:(-1,0),3:(0,-1)}\n",
        "pi = 0.25\n",
        "term_state = [(0,0),(grid_rows-1,grid_cols-1)]\n",
        "dir = {0:\"D\", 1:\"R\", 2:\"U\", 3:\"L\"}\n",
        "theta = 0.00001\n",
        "grid_4x4_PI(grid_rows, grid_cols, actions, pi,term_state, theta, action_map,dir)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0\n",
            "Evaluating Policy...\n",
            "[[  0.         -13.99993529 -19.99990698 -21.99989761]\n",
            " [-13.99993529 -17.9999206  -19.99991379 -19.99991477]\n",
            " [-19.99990698 -19.99991379 -17.99992725 -13.99994569]\n",
            " [-21.99989761 -19.99991477 -13.99994569   0.        ]]\n",
            "Improving Policy...\n",
            "[[[0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.   0.   0.   1.  ]]\n",
            "\n",
            " [[0.   0.   1.   0.  ]\n",
            "  [0.   0.   0.5  0.5 ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [1.   0.   0.   0.  ]]\n",
            "\n",
            " [[0.   0.   1.   0.  ]\n",
            "  [0.   0.   1.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [1.   0.   0.   0.  ]]\n",
            "\n",
            " [[0.   0.   1.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [0.25 0.25 0.25 0.25]]]\n",
            "Iteration 1\n",
            "Evaluating Policy...\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "Improving Policy...\n",
            "[[[0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.5  0.   0.   0.5 ]]\n",
            "\n",
            " [[0.   0.   1.   0.  ]\n",
            "  [0.   0.   0.5  0.5 ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [1.   0.   0.   0.  ]]\n",
            "\n",
            " [[0.   0.   1.   0.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.5  0.5  0.   0.  ]\n",
            "  [1.   0.   0.   0.  ]]\n",
            "\n",
            " [[0.   0.5  0.5  0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [0.25 0.25 0.25 0.25]]]\n",
            "Iteration 2\n",
            "Evaluating Policy...\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "Improving Policy...\n",
            "[[[0.25 0.25 0.25 0.25]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.   0.   0.   1.  ]\n",
            "  [0.5  0.   0.   0.5 ]]\n",
            "\n",
            " [[0.   0.   1.   0.  ]\n",
            "  [0.   0.   0.5  0.5 ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [1.   0.   0.   0.  ]]\n",
            "\n",
            " [[0.   0.   1.   0.  ]\n",
            "  [0.25 0.25 0.25 0.25]\n",
            "  [0.5  0.5  0.   0.  ]\n",
            "  [1.   0.   0.   0.  ]]\n",
            "\n",
            " [[0.   0.5  0.5  0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [0.   1.   0.   0.  ]\n",
            "  [0.25 0.25 0.25 0.25]]]\n",
            "\n",
            "Policy Converged\n",
            "\n",
            "#; L; L; DL\n",
            "\n",
            "U; UL; DRUL; D\n",
            "\n",
            "U; DRUL; DR; D\n",
            "\n",
            "RU; R; R; #\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6_wMaNRGwDj"
      },
      "source": [
        "### Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqH7VYPNLKYI"
      },
      "source": [
        "A similar process to as followed in Question 4 is repeated with a few tweaks to fit the gridworld in Ex 4.1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO_fCwX0GvTK"
      },
      "source": [
        "def grid_4x4_value_VI(v_s, grid_rows, grid_cols, actions,term_state, theta):\n",
        "  cnt = 0\n",
        "  while (True): \n",
        "    print(\"Iteration \"+str(cnt))\n",
        "    delta = 0\n",
        "    for i in range(grid_rows):\n",
        "      for j in range(grid_cols):\n",
        "        if (i,j) not in term_state:\n",
        "          old_v = copy.deepcopy(v_s[i][j])\n",
        "          # val = -sys.max - 1\n",
        "          returns = []\n",
        "          for a in actions:\n",
        "            s_dash, r = next_step_ex41((i,j), a, grid_rows)\n",
        "            returns.append(r + v_s[s_dash[0]][s_dash[1]])\n",
        "          v_s[i][j] = max(returns)\n",
        "          delta = max(delta, abs(old_v - v_s[i][j]))\n",
        "    print(v_s)\n",
        "    cnt+=1\n",
        "    if delta<theta:\n",
        "      return v_s    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZrB5TdIKFuQ",
        "outputId": "4ecc7465-6141-4092-c210-f7f5a98c9d83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "grid_rows = 4\n",
        "grid_cols = 4\n",
        "v_s = np.zeros((grid_rows, grid_cols))\n",
        "actions = [(1,0),(0,1),(-1,0),(0,-1)]\n",
        "theta = 0.00001\n",
        "term_state = [(0,0),(grid_rows-1,grid_cols-1)]\n",
        "v_star = grid_4x4_value_VI(v_s, grid_rows, grid_cols, actions,term_state, theta)\n",
        "# print(v_star)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0\n",
            "[[ 0. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.]\n",
            " [-1. -1. -1.  0.]]\n",
            "Iteration 1\n",
            "[[ 0. -1. -2. -2.]\n",
            " [-1. -2. -2. -2.]\n",
            " [-2. -2. -2. -1.]\n",
            " [-2. -2. -1.  0.]]\n",
            "Iteration 2\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "Iteration 3\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qh9VsfPOoaQ"
      },
      "source": [
        "def grid_4x4_policy_VI(v_star, pi_s, grid_rows, grid_cols, actions,term_state,dir):\n",
        "  for i in range(grid_rows):\n",
        "    for j in range(grid_cols):\n",
        "      if (i,j) not in term_state:\n",
        "        returns = []\n",
        "        for a in actions:\n",
        "          s_dash, r = next_step_ex41((i,j), a, grid_rows)\n",
        "          returns.append(r + v_star[s_dash[0]][s_dash[1]])\n",
        "        val = max(returns)\n",
        "        for k in range(len(returns)):\n",
        "          if val == returns[k]:\n",
        "              pi_s[i][j][k] = 1\n",
        "  print(pi_s)\n",
        "  fin_policy(pi_s,dir,grid_rows,grid_cols,term_state)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g8XuIAeUu8e",
        "outputId": "919edecd-f482-4630-9d06-1bf68695ca24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "grid_rows = 4\n",
        "grid_cols = 4\n",
        "actions = [(1,0),(0,1),(-1,0),(0,-1)]\n",
        "no_actions = len(actions)\n",
        "pi_s = np.zeros((grid_rows, grid_cols,no_actions))\n",
        "dir = {0:\"D\", 1:\"R\", 2:\"U\", 3:\"L\"}\n",
        "term_state = [(0,0),(grid_rows-1,grid_cols-1)]\n",
        "grid_4x4_policy_VI(v_star, pi_s, grid_rows, grid_cols, actions,term_state,dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1.]\n",
            "  [1. 0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 1. 0.]\n",
            "  [0. 0. 1. 1.]\n",
            "  [1. 1. 1. 1.]\n",
            "  [1. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 1. 0.]\n",
            "  [1. 1. 1. 1.]\n",
            "  [1. 1. 0. 0.]\n",
            "  [1. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 1. 0.]\n",
            "  [0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0.]]]\n",
            "#; L; L; DL\n",
            "\n",
            "U; UL; DRUL; D\n",
            "\n",
            "U; DRUL; DR; D\n",
            "\n",
            "RU; R; R; #\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezv_nfsblUE_"
      },
      "source": [
        "## Question 7: Policy Iteration, Book Exercise 4.7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP-9KC5cVxwy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}